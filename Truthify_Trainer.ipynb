{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrduarte/Documents/FCT/samsung/jupyter-env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-02-15 19:03:06.927318: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1739646186.940205  333389 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1739646186.944173  333389 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-15 19:03:06.960520: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import BertTokenizer, BertConfig , BertModel, DistilBertModel, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fact Analyses\n",
    "We need to classifie agruments as `true` or `false`.\n",
    "\n",
    "For that we will use the `Politifact Fact Check` and `Snopes Fact-news Data` datasets from the EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Data Processing\n",
    "This defines a Custom Dataset class to treat our data for Fact Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, labels, max_length=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.labels = labels\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.data[idx][\"sentence\"]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            sentence,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre Processing Data function\n",
    "We frist start to tranform all the labels in true or false.\n",
    "\n",
    "Only the labels `true` and `mostly-true` are considered as `true: 1` and the other labels are considered `false: 0`.\n",
    "\n",
    "One area of improvement in the future, is to use the other labels provided by the datasets of the websites (or even create custom ones).\n",
    "\n",
    "We also remove all the unwanted colums and add `None` to empty values of the remaining columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(dataset):\n",
    "  \n",
    "  #turn label either false or true\n",
    "  dataset['label']=[1 if x==\"true\"or x==\"mostly-true\" or x==\"half-true\" else 0 for x in dataset[1]] \n",
    "\n",
    "  #Dropping unwanted columns\n",
    "  dataset = dataset.drop(labels=[0,1,8,9,10,11,12] ,axis=1)\n",
    "  #Dealing with empty datapoints for metadata columns - subject, speaker, job, state,affiliation, context\n",
    "  meta = []\n",
    "  for i in range(len(dataset)):\n",
    "      subject = dataset[3][i]\n",
    "      if subject == 0:\n",
    "          subject = 'None'\n",
    "\n",
    "      speaker =  dataset[4][i]\n",
    "      if speaker == 0:\n",
    "          speaker = 'None'\n",
    "\n",
    "      job =  dataset[5][i]\n",
    "      if job == 0:\n",
    "          job = 'None'\n",
    "\n",
    "      state =  dataset[6][i]\n",
    "      if state == 0:\n",
    "          state = 'None'\n",
    "\n",
    "      affiliation =  dataset[7][i]\n",
    "      if affiliation == 0:\n",
    "          affiliation = 'None'\n",
    "\n",
    "      context =  dataset[13][i]\n",
    "      if context == 0 :\n",
    "          context = 'None'\n",
    "\n",
    "      meta.append(str(subject) + ' ' + str(speaker) + ' ' + str(job) + ' ' + str(state) + ' ' + str(affiliation) + ' ' + str(context)) #combining all the meta data columns into a single column\n",
    "  \n",
    "  #Adding cleaned and combined metadata column to the dataset\n",
    "  dataset[14] = meta\n",
    "  dataset[\"sentence\"] = dataset[14].astype('str')+\" \"+dataset[2] #Combining metadata and the text columns into single columns\n",
    "\n",
    "  dataset = dataset.drop([2,3,4,5,6,7,13,14], axis=1) #dropping metadata columns, as we have merged them into a single column\n",
    "  dataset.dropna() #Dropping if there are still any null values\n",
    "\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining BERT Train functions\n",
    "- Define BERT Model with classification head\n",
    "- Define the function to train the model\n",
    "- Define evaluate method\n",
    "- Define function to predict a verdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "from transformers import (\n",
    "    BertForSequenceClassification,    \n",
    "    BertTokenizer,\n",
    "    RobertaForSequenceClassification,\n",
    "    RobertaTokenizer,\n",
    "    AdamW)\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Classification Head\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, bert_model_name):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(bert_model_name)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, 2)  # Binary classification\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "        x = self.dropout(cls_embedding)\n",
    "        x = self.fc(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "\n",
    "# Train Function\n",
    "def train(model, data_loader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    progress_bar = tqdm(data_loader, desc=\"Training\", leave=True)\n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate(model, data_loader,device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actual_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "            actual_labels.extend(labels.cpu().tolist())\n",
    "    return accuracy_score(actual_labels, predictions), classification_report(actual_labels, predictions)\n",
    "\n",
    "# Label predict Function\n",
    "def predict_label(text, model, tokenizer, device, label_encoder, max_length=128):\n",
    "    model.eval()\n",
    "    encoding = tokenizer(text, return_tensors='pt', max_length=max_length, padding='max_length', truncation=True)\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pred_class = torch.argmax(outputs, dim=1).item()\n",
    "\n",
    "    return label_encoder.inverse_transform([pred_class])[0]  # Return actual label\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing\n",
    "We decided to use `distilbert-base-uncased` model with the following train paramenters\n",
    "\n",
    "**Jaime uma pequena explicaçao porque os usaste estes parametros e o destilbert**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define our models hyperparameters\n",
    "bert_model_name = 'distilbert-base-uncased'\n",
    "num_classes = 6\n",
    "max_length = 128\n",
    "batch_size = 8\n",
    "num_epochs = 10\n",
    "learning_rate = 2e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Both DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: 16921\n",
      "train data {'sentence': 'Marco Rubio 12/17/2014 other \"The reason why Cubans don\\'t have access to 21st century telecommunications — like smart phones, like access to the Internet — is because it is illegal in Cuba.\"', 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Loading the data\n",
    "\n",
    "# Load the LIAR dataset from TSV\n",
    "#df1 = pd.read_csv(os.path.join('Datasets','LIAR.tsv'), sep=\"\\t\", header=None)\n",
    "# Load the POLITIFACT dataset\n",
    "df2 = pd.read_json(os.path.join('Datasets','politifact_factcheck_data.json'), lines=True)\n",
    "\n",
    "#---------------Preprocess LIAR dataset----------------------\n",
    "#df1 = data_preprocessing(df1)\n",
    "\n",
    "#---------------Preprocess Politifact dataset----------------------\n",
    "df2[\"sentence\"] = df2[\"statement_originator\"] + \" said: \" + df2[\"statement\"] + \" (\" + df2[\"statement_date\"] + \" via \" + df2[\"statement_source\"] + \")\"\n",
    "\n",
    "# Encode labels: 1 for true, mostly-true, half-true, barely-true; 0 otherwise\n",
    "df2['label'] = df2['verdict'].apply(lambda x: 1 if x in ['true', 'mostly-true'] else 0)\n",
    "\n",
    "# Handle metadata and combine into a single column\n",
    "meta_columns = [\"statement_originator\", \"statement_date\", \"statement_source\"]\n",
    "df2['metadata'] = df2[meta_columns].fillna('None').astype(str).agg(' '.join, axis=1)\n",
    "\n",
    "# Create sentence column combining metadata and statement\n",
    "df2[\"sentence\"] = df2[\"metadata\"] + \" \" + df2[\"statement\"]\n",
    "\n",
    "# Drop original columns (optional)\n",
    "df2 = df2.drop(columns=meta_columns + [\"statement\", \"verdict\",\"metadata\",\"factcheck_analysis_link\",\"factchecker\", \"factcheck_date\"])\n",
    "\n",
    "\n",
    "#df = pd.concat([df1, df2])\n",
    "df = df2\n",
    "df.fillna(\"None\", inplace=True)\n",
    "df = df.dropna()\n",
    "\n",
    "#split data\n",
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(df[\"sentence\"].tolist(), df[\"label\"].tolist(), test_size=0.2)\t\n",
    "\n",
    "train_data = [{\"sentence\" : stm, \"label\" : vrd} for stm, vrd in zip(train_sentences, train_labels)]\n",
    "val_data = [{\"sentence\" : stm, \"label\" : vrd} for stm, vrd in zip(val_sentences, val_labels)]\n",
    "\n",
    "print(\"Train data size:\", len(train_data))\n",
    "print(\"train data\", train_data[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Barack Obama 6/11/2008 speech John McCain oppo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Matt Gaetz 6/7/2022 television \"Bennie Thompso...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kelly Ayotte 5/18/2016 news Says Maggie Hassan...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bloggers 2/1/2021 blog \"BUSTED: CDC Inflated C...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bobby Jindal 8/30/2015 television \"I'm the onl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>Dana Young 10/20/2016 other Says she \"voted fo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>Viral image 1/1/2021 social_media \"President-e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>Michael MacDonald 3/24/2021 statement Michigan...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>Vincent Fort 7/6/2010 other Says Democratic op...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>Victoria Taft 4/25/2011 social_media \"A $250 f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence  label\n",
       "0    Barack Obama 6/11/2008 speech John McCain oppo...      1\n",
       "1    Matt Gaetz 6/7/2022 television \"Bennie Thompso...      0\n",
       "2    Kelly Ayotte 5/18/2016 news Says Maggie Hassan...      1\n",
       "3    Bloggers 2/1/2021 blog \"BUSTED: CDC Inflated C...      0\n",
       "4    Bobby Jindal 8/30/2015 television \"I'm the onl...      0\n",
       "..                                                 ...    ...\n",
       "495  Dana Young 10/20/2016 other Says she \"voted fo...      0\n",
       "496  Viral image 1/1/2021 social_media \"President-e...      0\n",
       "497  Michael MacDonald 3/24/2021 statement Michigan...      0\n",
       "498  Vincent Fort 7/6/2010 other Says Democratic op...      0\n",
       "499  Victoria Taft 4/25/2011 social_media \"A $250 f...      0\n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name, do_lower_case=True)\n",
    "\n",
    "# Create DataLoaders, from pytorch (this gives me an error)\n",
    "train_dataset = CustomDataset(train_data, tokenizer,train_labels)\n",
    "val_dataset = CustomDataset(val_data, tokenizer,val_labels)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Set the device and load the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BERTClassifier(bert_model_name).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and save the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrduarte/Documents/FCT/samsung/jupyter-env/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2116/2116 [01:56<00:00, 18.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7455\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.94      0.84      3050\n",
      "           1       0.62      0.23      0.34      1181\n",
      "\n",
      "    accuracy                           0.75      4231\n",
      "   macro avg       0.69      0.59      0.59      4231\n",
      "weighted avg       0.72      0.75      0.70      4231\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2116/2116 [02:02<00:00, 17.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7317\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.84      0.82      3050\n",
      "           1       0.52      0.45      0.49      1181\n",
      "\n",
      "    accuracy                           0.73      4231\n",
      "   macro avg       0.66      0.65      0.65      4231\n",
      "weighted avg       0.72      0.73      0.73      4231\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2116/2116 [02:07<00:00, 16.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7299\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.83      0.82      3050\n",
      "           1       0.52      0.48      0.50      1181\n",
      "\n",
      "    accuracy                           0.73      4231\n",
      "   macro avg       0.66      0.65      0.66      4231\n",
      "weighted avg       0.72      0.73      0.73      4231\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2116/2116 [02:09<00:00, 16.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7143\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.79      0.80      3050\n",
      "           1       0.49      0.52      0.50      1181\n",
      "\n",
      "    accuracy                           0.71      4231\n",
      "   macro avg       0.65      0.65      0.65      4231\n",
      "weighted avg       0.72      0.71      0.72      4231\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2116/2116 [02:10<00:00, 16.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7393\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.89      0.83      3050\n",
      "           1       0.55      0.36      0.44      1181\n",
      "\n",
      "    accuracy                           0.74      4231\n",
      "   macro avg       0.67      0.62      0.63      4231\n",
      "weighted avg       0.72      0.74      0.72      4231\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2116/2116 [02:11<00:00, 16.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7374\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.87      0.83      3050\n",
      "           1       0.54      0.40      0.46      1181\n",
      "\n",
      "    accuracy                           0.74      4231\n",
      "   macro avg       0.66      0.63      0.64      4231\n",
      "weighted avg       0.72      0.74      0.72      4231\n",
      "\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2116/2116 [02:11<00:00, 16.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7329\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.84      0.82      3050\n",
      "           1       0.53      0.44      0.48      1181\n",
      "\n",
      "    accuracy                           0.73      4231\n",
      "   macro avg       0.66      0.64      0.65      4231\n",
      "weighted avg       0.72      0.73      0.73      4231\n",
      "\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2116/2116 [02:11<00:00, 16.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7379\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.84      0.82      3050\n",
      "           1       0.53      0.48      0.51      1181\n",
      "\n",
      "    accuracy                           0.74      4231\n",
      "   macro avg       0.67      0.66      0.66      4231\n",
      "weighted avg       0.73      0.74      0.73      4231\n",
      "\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2116/2116 [02:12<00:00, 15.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7393\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.87      0.83      3050\n",
      "           1       0.54      0.40      0.46      1181\n",
      "\n",
      "    accuracy                           0.74      4231\n",
      "   macro avg       0.67      0.64      0.65      4231\n",
      "weighted avg       0.72      0.74      0.73      4231\n",
      "\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2116/2116 [02:12<00:00, 15.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7317\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.84      0.82      3050\n",
      "           1       0.52      0.46      0.49      1181\n",
      "\n",
      "    accuracy                           0.73      4231\n",
      "   macro avg       0.66      0.65      0.65      4231\n",
      "weighted avg       0.72      0.73      0.73      4231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Optimizer and scheduler\n",
    "# AdamW is a class from the huggingface library\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Total number of training steps, being the number of batches * number of epochs\n",
    "total_steps = len(train_dataloader) * num_epochs \n",
    "num_warmup_steps = int(0.1 * total_steps)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        train(model, train_dataloader, optimizer, scheduler, device)\n",
    "        accuracy, report = evaluate(model, val_dataloader,device)\n",
    "        print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "        print(report)\n",
    "\n",
    "torch.save(model.state_dict(), os.path.join(\"models\",\"distilbert_fact_classifier.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate models perfomance\n",
    "To evaluate the model we gonna give some text samples from polifact website:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted sentiment: True\n",
      "Predicted sentiment: False\n",
      "Predicted sentiment: False\n"
     ]
    }
   ],
   "source": [
    "# Reload new model\n",
    "model.load_state_dict(torch.load('./models/distilbert_fact_classifier.pth', weights_only=False))\n",
    "model = model.to(device)\n",
    "\n",
    "# Function to test the sentiment of a text\n",
    "def test_sentiment(text, model, tokenizer):\n",
    "    label_encoder = {0: 'False', 1: 'True'}\n",
    "    sentiment = predict_label(text, model, tokenizer, device, label_encoder)\n",
    "    print(f\"Predicted sentiment: {sentiment}\")\n",
    "\n",
    "# Test sentiment prediction\n",
    "test_sentiment(\"The poverty rate decreased by 3% in the last two years\", model, tokenizer) # True\n",
    "test_sentiment(\"FEMA sent $59M LAST WEEK to luxury hotels in New York City to house illegal migrants… That money is meant for American disaster relief.\", model, tokenizer) # False\n",
    "test_sentiment(\"Former USAID Administrator Samantha Power’s net worth “skyrocketed” from $6.7 million to $30 million in three years.\", model, tokenizer) # False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
