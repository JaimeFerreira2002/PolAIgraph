{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import BertTokenizer, BertConfig , BertModel, DistilBertModel, BertForSequenceClassification, BertForNextSentencePrediction, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "from datasets import Dataset\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "# Set the device and load the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fact and Agrument Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part load dataset will be use to train and test BERT\n",
    "\n",
    "This dataset was created by our `llama_week_labeling.py` script using `unsloth/llama-3-8b-Instruct-bnb-4bit` model running locally\n",
    "\n",
    "This dataset need to be created using a week labeling, in this case using llama text generation model, since we can't find usable dataset. \n",
    "\n",
    "We use dataset from [`US Election 2020 - Presidential Debates`](https://www.kaggle.com/datasets/headsortails/us-election-2020-presidential-debates) collection and start to create the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>statement</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vice President Joe Biden</td>\n",
       "      <td>How you doing, man?</td>\n",
       "      <td>Argument</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vice President Joe Biden</td>\n",
       "      <td>I’m well.</td>\n",
       "      <td>Argument</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vice President Joe Biden</td>\n",
       "      <td>Well, first of all, thank you for doing this a...</td>\n",
       "      <td>Argument</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vice President Joe Biden</td>\n",
       "      <td>The American people have a right to have a say...</td>\n",
       "      <td>Argument</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vice President Joe Biden</td>\n",
       "      <td>Now, what’s at stake here is the President’s m...</td>\n",
       "      <td>Argument</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Vice President Joe Biden</td>\n",
       "      <td>And that ended when we, in fact, passed the Af...</td>\n",
       "      <td>Argument</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Vice President Joe Biden</td>\n",
       "      <td>He’s elected to the next election.</td>\n",
       "      <td>Argument</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Vice President Joe Biden</td>\n",
       "      <td>That’s simply not true.</td>\n",
       "      <td>Argument</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Vice President Joe Biden</td>\n",
       "      <td>Open discussion.</td>\n",
       "      <td>Argument</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Vice President Joe Biden</td>\n",
       "      <td>Number one, he knows what I proposed. What I p...</td>\n",
       "      <td>Argument</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    speaker  \\\n",
       "0  Vice President Joe Biden   \n",
       "1  Vice President Joe Biden   \n",
       "2  Vice President Joe Biden   \n",
       "3  Vice President Joe Biden   \n",
       "4  Vice President Joe Biden   \n",
       "5  Vice President Joe Biden   \n",
       "6  Vice President Joe Biden   \n",
       "7  Vice President Joe Biden   \n",
       "8  Vice President Joe Biden   \n",
       "9  Vice President Joe Biden   \n",
       "\n",
       "                                           statement     label  \n",
       "0                                How you doing, man?  Argument  \n",
       "1                                          I’m well.  Argument  \n",
       "2  Well, first of all, thank you for doing this a...  Argument  \n",
       "3  The American people have a right to have a say...  Argument  \n",
       "4  Now, what’s at stake here is the President’s m...  Argument  \n",
       "5  And that ended when we, in fact, passed the Af...  Argument  \n",
       "6                 He’s elected to the next election.  Argument  \n",
       "7                            That’s simply not true.  Argument  \n",
       "8                                   Open discussion.  Argument  \n",
       "9  Number one, he knows what I proposed. What I p...  Argument  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 563 entries, 0 to 562\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   speaker    563 non-null    object\n",
      " 1   statement  563 non-null    object\n",
      " 2   label      563 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 13.3+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Argument', 'Fact'], dtype=object)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>statement</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>Thank you, Susan. Well, the American people ha...</td>\n",
       "      <td>Argument</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>Can you imagine if you knew on January 28th, a...</td>\n",
       "      <td>Argument</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>… right to reelection based on this.</td>\n",
       "      <td>Argument</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>Absolutely. Whatever the vice president is cla...</td>\n",
       "      <td>Argument</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>No. But Susan, this is important. And I want t...</td>\n",
       "      <td>Argument</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>Mr. Vice President, I’m speaking.</td>\n",
       "      <td>Argument</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>I’m speaking.</td>\n",
       "      <td>Argument</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>Thank you. So I want to ask the American peopl...</td>\n",
       "      <td>Argument</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>… when your children-</td>\n",
       "      <td>Argument</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>… couldn’t see your parents because you were a...</td>\n",
       "      <td>Argument</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         speaker                                          statement     label\n",
       "0  Kamala Harris  Thank you, Susan. Well, the American people ha...  Argument\n",
       "1  Kamala Harris  Can you imagine if you knew on January 28th, a...  Argument\n",
       "2  Kamala Harris               … right to reelection based on this.  Argument\n",
       "3  Kamala Harris  Absolutely. Whatever the vice president is cla...  Argument\n",
       "4  Kamala Harris  No. But Susan, this is important. And I want t...  Argument\n",
       "5  Kamala Harris                  Mr. Vice President, I’m speaking.  Argument\n",
       "6  Kamala Harris                                      I’m speaking.  Argument\n",
       "7  Kamala Harris  Thank you. So I want to ask the American peopl...  Argument\n",
       "8  Kamala Harris                              … when your children-  Argument\n",
       "9  Kamala Harris  … couldn’t see your parents because you were a...  Argument"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 157 entries, 0 to 156\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   speaker    157 non-null    object\n",
      " 1   statement  157 non-null    object\n",
      " 2   label      157 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 3.8+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Argument', 'Fact'], dtype=object)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load datasets\n",
    "df_fact_agrument_1=pd.read_csv(os.path.join('Datasets','us_debates','fact-agrument','1st_presidential_fact_agument.csv'))\n",
    "df_fact_agrument_2=pd.read_csv(os.path.join('Datasets','us_debates','fact-agrument','vice_presidential_fact_agrument.csv'))\n",
    "\n",
    "# Some info about each dataset\n",
    "df_fact_agrument_1.head(10)\n",
    "df_fact_agrument_1.info()\n",
    "df_fact_agrument_1['label'].unique()\n",
    "\n",
    "df_fact_agrument_2.head(10)\n",
    "df_fact_agrument_2.info()\n",
    "df_fact_agrument_2['label'].unique()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also gonna include the `Politifact Fact Check` since the debate we get are more agruments that facts.\n",
    "\n",
    "We need to create a new dataset from this one with the same format as the one above, and concat the 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "Fact        1310\n",
       "Argument     710\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>statement</th>\n",
       "      <th>label</th>\n",
       "      <th>label_map</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mike Pence</td>\n",
       "      <td>And I’m going to speak up on behalf of what th...</td>\n",
       "      <td>Argument</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>\"Already we've identified $2 trillion in defic...</td>\n",
       "      <td>Fact</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>… for the Recovery Act that brought America ba...</td>\n",
       "      <td>Argument</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vice President Joe Biden</td>\n",
       "      <td>Because you in fact passed that, that was your...</td>\n",
       "      <td>Argument</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vice President Joe Biden</td>\n",
       "      <td>Mr. Vice-</td>\n",
       "      <td>Fact</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>David Beckham</td>\n",
       "      <td>\"Swaziland has the highest rate of HIV infecti...</td>\n",
       "      <td>Fact</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Facebook posts</td>\n",
       "      <td>Quotes Mike Pence as saying that people with p...</td>\n",
       "      <td>Fact</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Cindy O’Laughlin</td>\n",
       "      <td>\"When comparing state by state, the data clear...</td>\n",
       "      <td>Fact</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>\"We've recovered (from the recession) faster a...</td>\n",
       "      <td>Fact</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>President Donald J. Trump</td>\n",
       "      <td>A lot of people, between drugs and alcohol and...</td>\n",
       "      <td>Argument</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     speaker  \\\n",
       "0                 Mike Pence   \n",
       "1               Barack Obama   \n",
       "2              Kamala Harris   \n",
       "3   Vice President Joe Biden   \n",
       "4   Vice President Joe Biden   \n",
       "5              David Beckham   \n",
       "6             Facebook posts   \n",
       "7           Cindy O’Laughlin   \n",
       "8               Barack Obama   \n",
       "9  President Donald J. Trump   \n",
       "\n",
       "                                           statement     label  label_map  \n",
       "0  And I’m going to speak up on behalf of what th...  Argument          1  \n",
       "1  \"Already we've identified $2 trillion in defic...      Fact          0  \n",
       "2  … for the Recovery Act that brought America ba...  Argument          1  \n",
       "3  Because you in fact passed that, that was your...  Argument          1  \n",
       "4                                          Mr. Vice-      Fact          0  \n",
       "5  \"Swaziland has the highest rate of HIV infecti...      Fact          0  \n",
       "6  Quotes Mike Pence as saying that people with p...      Fact          0  \n",
       "7  \"When comparing state by state, the data clear...      Fact          0  \n",
       "8  \"We've recovered (from the recession) faster a...      Fact          0  \n",
       "9  A lot of people, between drugs and alcohol and...  Argument          1  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2020 entries, 0 to 2019\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   speaker    2020 non-null   object\n",
      " 1   statement  2020 non-null   object\n",
      " 2   label      2020 non-null   object\n",
      " 3   label_map  2020 non-null   int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 63.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df_polifact = pd.read_json(os.path.join('Datasets','politifact_factcheck_data.json'), lines=True)\n",
    "# Cut dataset to randomly 1300 rows, to prevent to many facts\n",
    "df_polifact = df_polifact.sample(n=1300, random_state=42)\n",
    "\n",
    "# Dataset for training\n",
    "df_trainer_final=pd.DataFrame(columns=[\"speaker\", \"statement\", \"label\"])\n",
    "\n",
    "# We add the speaker but is not needed \n",
    "# (the speaker information was not use to prevent biases)\n",
    "df_trainer_final[\"speaker\"]=df_polifact[\"statement_originator\"]\n",
    "df_trainer_final[\"statement\"]=df_polifact[\"statement\"]\n",
    "df_trainer_final[\"label\"]=\"Fact\"\n",
    "\n",
    "# Concat with the 2 early datasets and shuffe dataset\n",
    "df_trainer_final = pd.concat([df_trainer_final, df_fact_agrument_1, df_fact_agrument_2])\n",
    "df_trainer_final = df_trainer_final.sample(frac=1, random_state=50).reset_index(drop=True)\n",
    "\n",
    "# At last we gonna map the lables\n",
    "label2id = {\"Fact\": 0, \"Argument\": 1}\n",
    "id2label = {v: k for k, v in label2id.items()}  # Reverse mapping\n",
    "df_trainer_final[\"label_map\"]=df_trainer_final[\"label\"].map(label2id)\n",
    "\n",
    "df_trainer_final.head(10)\n",
    "df_trainer_final.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing\n",
    "We decided to use `distilbert-base-uncased` model with the following train hyperparamenters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define our models hyperparameters\n",
    "bert_model_name = 'distilbert-base-uncased' # smaller bert model\n",
    "num_classes = 6\n",
    "max_length = 128\n",
    "batch_size = 8\n",
    "num_epochs = 10\n",
    "learning_rate = 2e-5\n",
    "warmup_steps=500  # number of warmup steps for learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fact_agrument_clasifier(model, statement_list: list, labels_list: list, tokenizer):\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"statement\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    # Do a split with stratify to preserve class distribution\n",
    "    x_train, x_test, y_train, y_test = train_test_split(statement_list, labels_list, test_size=0.35, stratify=labels_list)\n",
    "\n",
    "    # Create new train and test dataset\n",
    "    train_data = Dataset.from_dict({\n",
    "        'statement': [t[0] for t in x_train],\n",
    "        'label': [int(label) for label in y_train]\n",
    "    })\n",
    "\n",
    "    test_data = Dataset.from_dict({\n",
    "        'statement': [t[0] for t in x_test],\n",
    "        'label': [int(label) for label in y_test]\n",
    "    })\n",
    "\n",
    "    # Map the train and text dataset with the tokeneizer\n",
    "    train_data = train_data.map(tokenize_function, batched=True)\n",
    "    test_data = test_data.map(tokenize_function, batched=True)\n",
    "\n",
    "    # Remove original text columns (keep only tokenized inputs)\n",
    "    train_data = train_data.remove_columns([\"statement\"])\n",
    "    test_data = test_data.remove_columns([\"statement\"])\n",
    "\n",
    "    # Use acuracy as the metric\n",
    "    def compute_metrics(eval_pred):\n",
    "        metric = evaluate.load(\"accuracy\")\n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        accuracy = metric.compute(predictions=predictions, references=labels)\n",
    "        return accuracy\n",
    "    \n",
    "    # Define training agruments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        num_train_epochs=num_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        warmup_steps=warmup_steps,\n",
    "        save_total_limit=2,  # limit the total amount of checkpoints, delete the older checkpoints\n",
    "        eval_steps=500, # Perform evaluation every 100 steps\n",
    "        save_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        metric_for_best_model=\"accuracy\",  # Metric to use for selecting the best model\n",
    "        greater_is_better=True,  # Whether a higher value of the metric is better\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_data,  # training data\n",
    "        eval_dataset=test_data,  # evaluation data\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model(\"./models/distilbert_fact_agrument_classifier\")\n",
    "    tokenizer.save_pretrained(\"./models/distilbert_fact_agrument_classifier\")\n",
    "\n",
    "    # Reload with new model\n",
    "    model = BertForNextSentencePrediction.from_pretrained(\"./models/distilbert_fact_agrument_classifier\").to(device)\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"./models/distilbert_fact_agrument_classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "\n",
      "\u001b[A\n",
      "Map: 100%|██████████| 1313/1313 [00:00<00:00, 3344.42 examples/s]\n",
      "\n",
      "\u001b[A\n",
      "Map: 100%|██████████| 707/707 [00:00<00:00, 3278.37 examples/s]\n",
      " 19%|█▊        | 306/1650 [04:17<18:49,  1.19it/s]\n",
      "100%|█████████▉| 329/330 [02:32<00:00,  2.12it/s]\n",
      "100%|██████████| 330/330 [02:35<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 155.891, 'train_samples_per_second': 16.845, 'train_steps_per_second': 2.117, 'train_loss': 0.49594999371152937, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForNextSentencePrediction were not initialized from the model checkpoint at ./models/distilbert_fact_agrument_classifier and are newly initialized: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load default model\n",
    "config = BertConfig.from_pretrained(bert_model_name, num_labels=len(label2id), label2id=label2id, id2label=id2label)\n",
    "model = BertForSequenceClassification.from_pretrained(bert_model_name, config=config).to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "\n",
    "# Training\n",
    "statement_train_list=df_trainer_final[\"statement\"].to_list()\n",
    "labels_list=df_trainer_final[\"label_map\"].to_list()\n",
    "\n",
    "train_fact_agrument_clasifier(model, statement_train_list, labels_list, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate models perfomance\n",
    "To evaluate the model we gonna give some text samples from polifact website and some text from the debates dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment for text: 'The poverty rate decreased by 3% in the last two years' is Fact\n",
      "Sentiment for text: 'FEMA sent $59M LAST WEEK to luxury hotels in New York City to house illegal migrants… That money is meant for American disaster relief.' is Fact\n",
      "Sentiment for text: 'But many people are catching it. Many people are getting this disease that was sent to us by China, and it shouldn’t have been allowed to happen.' is Fact\n",
      "tensor([0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Reload new model\n",
    "model = BertForSequenceClassification.from_pretrained(\"./models/distilbert_fact_agrument_classifier\").to(device)\n",
    "\n",
    "# Function to test the sentiment of a text\n",
    "def test_model(text, model, tokenizer):\n",
    "    labels = {0: \"Fact\", 1: \"Argument\"}\n",
    "    # Tokenize the text and add padding/truncation\n",
    "    encoding = tokenizer(text, return_tensors='pt')\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    # Make the model prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1).item()  # Convert logits to predicted class index\n",
    "\n",
    "    # Output the prediction label based on the model's class index\n",
    "    predicted_label = labels[predictions]  # Map the predicted index to the label\n",
    "    print(f\"Sentiment for text: '{text}' is {predicted_label}\")\n",
    "\n",
    "\n",
    "# Test sentiment prediction\n",
    "test_model(\"The poverty rate decreased by 3% in the last two years\", model, tokenizer) # Fact\n",
    "test_model(\"FEMA sent $59M LAST WEEK to luxury hotels in New York City to house illegal migrants… That money is meant for American disaster relief.\", model, tokenizer) # Agrument\n",
    "test_model(\"But many people are catching it. Many people are getting this disease that was sent to us by China, and it shouldn’t have been allowed to happen.\", model, tokenizer) # Agrument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fact Analyses\n",
    "We need to classifie agruments as `true` or `false`.\n",
    "\n",
    "For that we will use the `Politifact Fact Check` and `Snopes Fact-news Data` datasets from the EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Data Processing\n",
    "This defines a Custom Dataset class to treat our data for Fact Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, labels, max_length=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.labels = labels\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.data[idx][\"sentence\"]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            sentence,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre Processing Data function\n",
    "We frist start to tranform all the labels in true or false.\n",
    "\n",
    "Only the labels `true` and `mostly-true` are considered as `true: 1` and the other labels are considered `false: 0`.\n",
    "\n",
    "One area of improvement in the future, is to use the other labels provided by the datasets of the websites (or even create custom ones).\n",
    "\n",
    "We also remove all the unwanted colums and add `None` to empty values of the remaining columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(dataset):\n",
    "  \n",
    "  #turn label either false or true\n",
    "  dataset['label']=[1 if x==\"true\"or x==\"mostly-true\" or x==\"half-true\" else 0 for x in dataset[1]] \n",
    "\n",
    "  #Dropping unwanted columns\n",
    "  dataset = dataset.drop(labels=[0,1,8,9,10,11,12] ,axis=1)\n",
    "  #Dealing with empty datapoints for metadata columns - subject, speaker, job, state,affiliation, context\n",
    "  meta = []\n",
    "  for i in range(len(dataset)):\n",
    "      subject = dataset[3][i]\n",
    "      if subject == 0:\n",
    "          subject = 'None'\n",
    "\n",
    "      speaker =  dataset[4][i]\n",
    "      if speaker == 0:\n",
    "          speaker = 'None'\n",
    "\n",
    "      job =  dataset[5][i]\n",
    "      if job == 0:\n",
    "          job = 'None'\n",
    "\n",
    "      state =  dataset[6][i]\n",
    "      if state == 0:\n",
    "          state = 'None'\n",
    "\n",
    "      affiliation =  dataset[7][i]\n",
    "      if affiliation == 0:\n",
    "          affiliation = 'None'\n",
    "\n",
    "      context =  dataset[13][i]\n",
    "      if context == 0 :\n",
    "          context = 'None'\n",
    "\n",
    "      meta.append(str(subject) + ' ' + str(speaker) + ' ' + str(job) + ' ' + str(state) + ' ' + str(affiliation) + ' ' + str(context)) #combining all the meta data columns into a single column\n",
    "  \n",
    "  #Adding cleaned and combined metadata column to the dataset\n",
    "  dataset[14] = meta\n",
    "  dataset[\"sentence\"] = dataset[14].astype('str')+\" \"+dataset[2] #Combining metadata and the text columns into single columns\n",
    "\n",
    "  dataset = dataset.drop([2,3,4,5,6,7,13,14], axis=1) #dropping metadata columns, as we have merged them into a single column\n",
    "  dataset.dropna() #Dropping if there are still any null values\n",
    "\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining BERT Train functions\n",
    "- Define BERT Model with classification head\n",
    "- Define the function to train the model\n",
    "- Define evaluate method\n",
    "- Define function to predict a verdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "from transformers import (\n",
    "    BertForSequenceClassification,    \n",
    "    BertTokenizer,\n",
    "    RobertaForSequenceClassification,\n",
    "    RobertaTokenizer,\n",
    "    AdamW)\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Classification Head\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, bert_model_name):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(bert_model_name)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, 2)  # Binary classification\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "        x = self.dropout(cls_embedding)\n",
    "        x = self.fc(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "\n",
    "# Train Function\n",
    "def train(model, data_loader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    progress_bar = tqdm(data_loader, desc=\"Training\", leave=True)\n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate_model(model, data_loader,device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actual_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "            actual_labels.extend(labels.cpu().tolist())\n",
    "    return accuracy_score(actual_labels, predictions), classification_report(actual_labels, predictions)\n",
    "\n",
    "# Label predict Function\n",
    "def predict_label(text, model, tokenizer, device, label_encoder, max_length=128):\n",
    "    model.eval()\n",
    "    encoding = tokenizer(text, return_tensors='pt', max_length=max_length, padding='max_length', truncation=True)\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pred_class = torch.argmax(outputs, dim=1).item()\n",
    "\n",
    "    return label_encoder.inverse_transform([pred_class])[0]  # Return actual label\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing\n",
    "We decided to use `distilbert-base-uncased` model with the following train hyperparamenters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define our models hyperparameters\n",
    "bert_model_name = 'distilbert-base-uncased'\n",
    "num_classes = 6\n",
    "max_length = 128\n",
    "batch_size = 8\n",
    "num_epochs = 10\n",
    "learning_rate = 2e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Both DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: 16921\n",
      "train data {'sentence': 'Donald Trump 2/25/2016 speech Says Ted Cruz\\xa0\"said I was in favor in Libya. I never discussed that subject.\"', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Loading the data\n",
    "\n",
    "# Load the LIAR dataset from TSV\n",
    "#df1 = pd.read_csv(os.path.join('Datasets','LIAR.tsv'), sep=\"\\t\", header=None)\n",
    "# Load the POLITIFACT dataset\n",
    "df2 = pd.read_json(os.path.join('Datasets','politifact_factcheck_data.json'), lines=True)\n",
    "\n",
    "#---------------Preprocess LIAR dataset----------------------\n",
    "#df1 = data_preprocessing(df1)\n",
    "\n",
    "#---------------Preprocess Politifact dataset----------------------\n",
    "df2[\"sentence\"] = df2[\"statement_originator\"] + \" said: \" + df2[\"statement\"] + \" (\" + df2[\"statement_date\"] + \" via \" + df2[\"statement_source\"] + \")\"\n",
    "\n",
    "# Encode labels: 1 for true, mostly-true, half-true, barely-true; 0 otherwise\n",
    "df2['label'] = df2['verdict'].apply(lambda x: 1 if x in ['true', 'mostly-true'] else 0)\n",
    "\n",
    "# Handle metadata and combine into a single column\n",
    "meta_columns = [\"statement_originator\", \"statement_date\", \"statement_source\"]\n",
    "df2['metadata'] = df2[meta_columns].fillna('None').astype(str).agg(' '.join, axis=1)\n",
    "\n",
    "# Create sentence column combining metadata and statement\n",
    "df2[\"sentence\"] = df2[\"metadata\"] + \" \" + df2[\"statement\"]\n",
    "\n",
    "# Drop original columns (optional)\n",
    "df2 = df2.drop(columns=meta_columns + [\"statement\", \"verdict\",\"metadata\",\"factcheck_analysis_link\",\"factchecker\", \"factcheck_date\"])\n",
    "\n",
    "\n",
    "#df = pd.concat([df1, df2])\n",
    "df = df2\n",
    "df.fillna(\"None\", inplace=True)\n",
    "df = df.dropna()\n",
    "\n",
    "#split data\n",
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(df[\"sentence\"].tolist(), df[\"label\"].tolist(), test_size=0.2)\t\n",
    "\n",
    "train_data = [{\"sentence\" : stm, \"label\" : vrd} for stm, vrd in zip(train_sentences, train_labels)]\n",
    "val_data = [{\"sentence\" : stm, \"label\" : vrd} for stm, vrd in zip(val_sentences, val_labels)]\n",
    "\n",
    "print(\"Train data size:\", len(train_data))\n",
    "print(\"train data\", train_data[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Barack Obama 6/11/2008 speech John McCain oppo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Matt Gaetz 6/7/2022 television \"Bennie Thompso...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kelly Ayotte 5/18/2016 news Says Maggie Hassan...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bloggers 2/1/2021 blog \"BUSTED: CDC Inflated C...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bobby Jindal 8/30/2015 television \"I'm the onl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>Dana Young 10/20/2016 other Says she \"voted fo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>Viral image 1/1/2021 social_media \"President-e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>Michael MacDonald 3/24/2021 statement Michigan...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>Vincent Fort 7/6/2010 other Says Democratic op...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>Victoria Taft 4/25/2011 social_media \"A $250 f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence  label\n",
       "0    Barack Obama 6/11/2008 speech John McCain oppo...      1\n",
       "1    Matt Gaetz 6/7/2022 television \"Bennie Thompso...      0\n",
       "2    Kelly Ayotte 5/18/2016 news Says Maggie Hassan...      1\n",
       "3    Bloggers 2/1/2021 blog \"BUSTED: CDC Inflated C...      0\n",
       "4    Bobby Jindal 8/30/2015 television \"I'm the onl...      0\n",
       "..                                                 ...    ...\n",
       "495  Dana Young 10/20/2016 other Says she \"voted fo...      0\n",
       "496  Viral image 1/1/2021 social_media \"President-e...      0\n",
       "497  Michael MacDonald 3/24/2021 statement Michigan...      0\n",
       "498  Vincent Fort 7/6/2010 other Says Democratic op...      0\n",
       "499  Victoria Taft 4/25/2011 social_media \"A $250 f...      0\n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name, do_lower_case=True)\n",
    "\n",
    "# Create DataLoaders, from pytorch (this gives me an error)\n",
    "train_dataset = CustomDataset(train_data, tokenizer,train_labels)\n",
    "val_dataset = CustomDataset(val_data, tokenizer,val_labels)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Set the device and load the model\n",
    "model = BERTClassifier(bert_model_name).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and save the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2116/2116 [01:59<00:00, 17.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7483\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.93      0.84      3042\n",
      "           1       0.61      0.28      0.38      1189\n",
      "\n",
      "    accuracy                           0.75      4231\n",
      "   macro avg       0.69      0.61      0.61      4231\n",
      "weighted avg       0.72      0.75      0.71      4231\n",
      "\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2116/2116 [02:11<00:00, 16.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7388\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.88      0.83      3042\n",
      "           1       0.55      0.39      0.45      1189\n",
      "\n",
      "    accuracy                           0.74      4231\n",
      "   macro avg       0.67      0.63      0.64      4231\n",
      "weighted avg       0.72      0.74      0.72      4231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Optimizer and scheduler\n",
    "# AdamW is a class from the huggingface library\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Total number of training steps, being the number of batches * number of epochs\n",
    "total_steps = len(train_dataloader) * num_epochs \n",
    "num_warmup_steps = int(0.1 * total_steps)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        train(model, train_dataloader, optimizer, scheduler, device)\n",
    "        accuracy, report = evaluate_model(model, val_dataloader,device)\n",
    "        print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "        print(report)\n",
    "\n",
    "torch.save(model.state_dict(), os.path.join(\"models\",\"distilbert_fact_classifier.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate models perfomance\n",
    "To evaluate the model we gonna give some text samples from polifact website:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted sentiment: True\n",
      "Predicted sentiment: False\n",
      "Predicted sentiment: False\n"
     ]
    }
   ],
   "source": [
    "# Reload new model\n",
    "model.load_state_dict(torch.load('./models/distilbert_fact_classifier.pth', weights_only=False))\n",
    "model = model.to(device)\n",
    "\n",
    "# Function to test the sentiment of a text\n",
    "def test_sentiment(text, model, tokenizer):\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(['False', 'True'])\n",
    "    sentiment = predict_label(text, model, tokenizer, device, label_encoder)\n",
    "    print(f\"Predicted sentiment: {sentiment}\")\n",
    "\n",
    "# Test sentiment prediction\n",
    "test_sentiment(\"The poverty rate decreased by 3% in the last two years\", model, tokenizer) # True\n",
    "test_sentiment(\"FEMA sent $59M LAST WEEK to luxury hotels in New York City to house illegal migrants… That money is meant for American disaster relief.\", model, tokenizer) # False\n",
    "test_sentiment(\"Former USAID Administrator Samantha Power’s net worth “skyrocketed” from $6.7 million to $30 million in three years.\", model, tokenizer) # False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agrument Analyses\n",
    "This part load dataset will be use to train and test BERT\n",
    "\n",
    "This dataset was also created by our `llama_week_labeling.py` script.\n",
    "\n",
    "We use dataset from [`US Election 2020 - Presidential Debates`](https://www.kaggle.com/datasets/headsortails/us-election-2020-presidential-debates) collection and start to create the labels:\n",
    "- `Restatement`: The second statement restates or reinforces the first.\n",
    "- `Counterargument`: The second statement opposes the first.\n",
    "- `Neutral`: No clear relationship between the statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 745 entries, 0 to 744\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   speaker1    745 non-null    object\n",
      " 1   statement1  745 non-null    object\n",
      " 2   speaker2    745 non-null    object\n",
      " 3   statement2  745 non-null    object\n",
      " 4   label       745 non-null    object\n",
      " 5   label_map   745 non-null    int64 \n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 35.1+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker1</th>\n",
       "      <th>statement1</th>\n",
       "      <th>speaker2</th>\n",
       "      <th>statement2</th>\n",
       "      <th>label</th>\n",
       "      <th>label_map</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vice President Joe Biden</td>\n",
       "      <td>How you doing, man?</td>\n",
       "      <td>President Donald J. Trump</td>\n",
       "      <td>How are you doing?</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vice President Joe Biden</td>\n",
       "      <td>How you doing, man?</td>\n",
       "      <td>President Donald J. Trump</td>\n",
       "      <td>Thank you very much, Chris. I will tell you ve...</td>\n",
       "      <td>Counterargument</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vice President Joe Biden</td>\n",
       "      <td>I’m well.</td>\n",
       "      <td>President Donald J. Trump</td>\n",
       "      <td>How are you doing?</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vice President Joe Biden</td>\n",
       "      <td>I’m well.</td>\n",
       "      <td>President Donald J. Trump</td>\n",
       "      <td>Thank you very much, Chris. I will tell you ve...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vice President Joe Biden</td>\n",
       "      <td>I’m well.</td>\n",
       "      <td>President Donald J. Trump</td>\n",
       "      <td>And we won the election and therefore we have ...</td>\n",
       "      <td>Counterargument</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   speaker1           statement1                   speaker2  \\\n",
       "0  Vice President Joe Biden  How you doing, man?  President Donald J. Trump   \n",
       "1  Vice President Joe Biden  How you doing, man?  President Donald J. Trump   \n",
       "2  Vice President Joe Biden            I’m well.  President Donald J. Trump   \n",
       "3  Vice President Joe Biden            I’m well.  President Donald J. Trump   \n",
       "4  Vice President Joe Biden            I’m well.  President Donald J. Trump   \n",
       "\n",
       "                                          statement2            label  \\\n",
       "0                                 How are you doing?          Neutral   \n",
       "1  Thank you very much, Chris. I will tell you ve...  Counterargument   \n",
       "2                                 How are you doing?          Neutral   \n",
       "3  Thank you very much, Chris. I will tell you ve...          Neutral   \n",
       "4  And we won the election and therefore we have ...  Counterargument   \n",
       "\n",
       "   label_map  \n",
       "0          0  \n",
       "1          1  \n",
       "2          0  \n",
       "3          0  \n",
       "4          1  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 416 entries, 0 to 415\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   speaker1    416 non-null    object\n",
      " 1   statement1  416 non-null    object\n",
      " 2   speaker2    416 non-null    object\n",
      " 3   statement2  416 non-null    object\n",
      " 4   label       416 non-null    object\n",
      " 5   label_map   416 non-null    int64 \n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 19.6+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker1</th>\n",
       "      <th>statement1</th>\n",
       "      <th>speaker2</th>\n",
       "      <th>statement2</th>\n",
       "      <th>label</th>\n",
       "      <th>label_map</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Savannah Guthrie</td>\n",
       "      <td>It’s nothing but noise. What? Okay. All right,...</td>\n",
       "      <td>President Trump</td>\n",
       "      <td>I’m feeling great, I don’t know about you. How...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Savannah Guthrie</td>\n",
       "      <td>It’s nothing but noise. What? Okay. All right,...</td>\n",
       "      <td>President Trump</td>\n",
       "      <td>It’s great to be back in my home state, Florid...</td>\n",
       "      <td>Counterargument</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Savannah Guthrie</td>\n",
       "      <td>Tonight, Donald Trump in the arena. His first ...</td>\n",
       "      <td>President Trump</td>\n",
       "      <td>I’m feeling great, I don’t know about you. How...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Savannah Guthrie</td>\n",
       "      <td>Tonight, Donald Trump in the arena. His first ...</td>\n",
       "      <td>President Trump</td>\n",
       "      <td>It’s great to be back in my home state, Florid...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Savannah Guthrie</td>\n",
       "      <td>Tonight, Donald Trump in the arena. His first ...</td>\n",
       "      <td>President Trump</td>\n",
       "      <td>My goal is to fight for you and fight for your...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           speaker1                                         statement1  \\\n",
       "0  Savannah Guthrie  It’s nothing but noise. What? Okay. All right,...   \n",
       "1  Savannah Guthrie  It’s nothing but noise. What? Okay. All right,...   \n",
       "2  Savannah Guthrie  Tonight, Donald Trump in the arena. His first ...   \n",
       "3  Savannah Guthrie  Tonight, Donald Trump in the arena. His first ...   \n",
       "4  Savannah Guthrie  Tonight, Donald Trump in the arena. His first ...   \n",
       "\n",
       "          speaker2                                         statement2  \\\n",
       "0  President Trump  I’m feeling great, I don’t know about you. How...   \n",
       "1  President Trump  It’s great to be back in my home state, Florid...   \n",
       "2  President Trump  I’m feeling great, I don’t know about you. How...   \n",
       "3  President Trump  It’s great to be back in my home state, Florid...   \n",
       "4  President Trump  My goal is to fight for you and fight for your...   \n",
       "\n",
       "             label  label_map  \n",
       "0          Neutral          0  \n",
       "1  Counterargument          1  \n",
       "2          Neutral          0  \n",
       "3          Neutral          0  \n",
       "4          Neutral          0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the labels\n",
    "label2id = {\"Neutral\": 0, \"Counterargument\": 1, \"Restatement\": 2}\n",
    "id2label = {v: k for k, v in label2id.items()}  # Reverse mapping\n",
    "\n",
    "# Load datasets for training\n",
    "df_agrument_1=pd.read_csv(os.path.join('Datasets','us_debates','agrument','1st_presidential_agrument.csv'))\n",
    "# Only one speaker\n",
    "df_agrument_2=pd.read_csv(os.path.join('Datasets','us_debates','agrument','trump_town_hall_agrument.csv'))\n",
    "\n",
    "# Map the labels\n",
    "df_agrument_1[\"label_map\"]=df_agrument_1[\"label\"].map(label2id)\n",
    "df_agrument_2[\"label_map\"]=df_agrument_2[\"label\"].map(label2id)\n",
    "\n",
    "# Print some dataset information\n",
    "df_agrument_1.info()\n",
    "df_agrument_1.head()\n",
    "df_agrument_2.info()\n",
    "df_agrument_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker1</th>\n",
       "      <th>statement1</th>\n",
       "      <th>speaker2</th>\n",
       "      <th>statement2</th>\n",
       "      <th>label</th>\n",
       "      <th>label_map</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vice President Joe Biden</td>\n",
       "      <td>How you doing, man?</td>\n",
       "      <td>President Donald J. Trump</td>\n",
       "      <td>How are you doing?</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vice President Joe Biden</td>\n",
       "      <td>How you doing, man?</td>\n",
       "      <td>President Donald J. Trump</td>\n",
       "      <td>Thank you very much, Chris. I will tell you ve...</td>\n",
       "      <td>Counterargument</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vice President Joe Biden</td>\n",
       "      <td>I’m well.</td>\n",
       "      <td>President Donald J. Trump</td>\n",
       "      <td>How are you doing?</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vice President Joe Biden</td>\n",
       "      <td>I’m well.</td>\n",
       "      <td>President Donald J. Trump</td>\n",
       "      <td>Thank you very much, Chris. I will tell you ve...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vice President Joe Biden</td>\n",
       "      <td>I’m well.</td>\n",
       "      <td>President Donald J. Trump</td>\n",
       "      <td>And we won the election and therefore we have ...</td>\n",
       "      <td>Counterargument</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Vice President Joe Biden</td>\n",
       "      <td>Well, first of all, thank you for doing this a...</td>\n",
       "      <td>President Donald J. Trump</td>\n",
       "      <td>Thank you very much, Chris. I will tell you ve...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Vice President Joe Biden</td>\n",
       "      <td>Well, first of all, thank you for doing this a...</td>\n",
       "      <td>President Donald J. Trump</td>\n",
       "      <td>And we won the election and therefore we have ...</td>\n",
       "      <td>Counterargument</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Vice President Joe Biden</td>\n",
       "      <td>Well, first of all, thank you for doing this a...</td>\n",
       "      <td>President Donald J. Trump</td>\n",
       "      <td>Thank you, Joe.</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Vice President Joe Biden</td>\n",
       "      <td>The American people have a right to have a say...</td>\n",
       "      <td>President Donald J. Trump</td>\n",
       "      <td>And we won the election and therefore we have ...</td>\n",
       "      <td>Counterargument</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Vice President Joe Biden</td>\n",
       "      <td>The American people have a right to have a say...</td>\n",
       "      <td>President Donald J. Trump</td>\n",
       "      <td>Thank you, Joe.</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   speaker1  \\\n",
       "0  Vice President Joe Biden   \n",
       "1  Vice President Joe Biden   \n",
       "2  Vice President Joe Biden   \n",
       "3  Vice President Joe Biden   \n",
       "4  Vice President Joe Biden   \n",
       "5  Vice President Joe Biden   \n",
       "6  Vice President Joe Biden   \n",
       "7  Vice President Joe Biden   \n",
       "8  Vice President Joe Biden   \n",
       "9  Vice President Joe Biden   \n",
       "\n",
       "                                          statement1  \\\n",
       "0                                How you doing, man?   \n",
       "1                                How you doing, man?   \n",
       "2                                          I’m well.   \n",
       "3                                          I’m well.   \n",
       "4                                          I’m well.   \n",
       "5  Well, first of all, thank you for doing this a...   \n",
       "6  Well, first of all, thank you for doing this a...   \n",
       "7  Well, first of all, thank you for doing this a...   \n",
       "8  The American people have a right to have a say...   \n",
       "9  The American people have a right to have a say...   \n",
       "\n",
       "                    speaker2  \\\n",
       "0  President Donald J. Trump   \n",
       "1  President Donald J. Trump   \n",
       "2  President Donald J. Trump   \n",
       "3  President Donald J. Trump   \n",
       "4  President Donald J. Trump   \n",
       "5  President Donald J. Trump   \n",
       "6  President Donald J. Trump   \n",
       "7  President Donald J. Trump   \n",
       "8  President Donald J. Trump   \n",
       "9  President Donald J. Trump   \n",
       "\n",
       "                                          statement2            label  \\\n",
       "0                                 How are you doing?          Neutral   \n",
       "1  Thank you very much, Chris. I will tell you ve...  Counterargument   \n",
       "2                                 How are you doing?          Neutral   \n",
       "3  Thank you very much, Chris. I will tell you ve...          Neutral   \n",
       "4  And we won the election and therefore we have ...  Counterargument   \n",
       "5  Thank you very much, Chris. I will tell you ve...          Neutral   \n",
       "6  And we won the election and therefore we have ...  Counterargument   \n",
       "7                                    Thank you, Joe.          Neutral   \n",
       "8  And we won the election and therefore we have ...  Counterargument   \n",
       "9                                    Thank you, Joe.          Neutral   \n",
       "\n",
       "   label_map  \n",
       "0          0  \n",
       "1          1  \n",
       "2          0  \n",
       "3          0  \n",
       "4          1  \n",
       "5          0  \n",
       "6          1  \n",
       "7          0  \n",
       "8          1  \n",
       "9          0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1161 entries, 0 to 415\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   speaker1    1161 non-null   object\n",
      " 1   statement1  1161 non-null   object\n",
      " 2   speaker2    1161 non-null   object\n",
      " 3   statement2  1161 non-null   object\n",
      " 4   label       1161 non-null   object\n",
      " 5   label_map   1161 non-null   int64 \n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 63.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# Concat the 2 early datasets\n",
    "df_trainer_final = pd.concat([df_agrument_1, df_agrument_2])\n",
    "df_trainer_final.head(10)\n",
    "df_trainer_final.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing\n",
    "We decided to use `distilbert-base-uncased` model with the following train hyperparamenters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define our models hyperparameters\n",
    "bert_model_name = 'distilbert-base-uncased' # smaller bert model\n",
    "num_classes = 6\n",
    "max_length = 128\n",
    "batch_size = 8\n",
    "num_epochs = 10\n",
    "learning_rate = 2e-5\n",
    "warmup_steps=500  # number of warmup steps for learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "Map: 100%|██████████| 754/754 [00:00<00:00, 1104.59 examples/s]\n",
      "Map: 100%|██████████| 407/407 [00:00<00:00, 1085.69 examples/s]\n",
      "100%|██████████| 190/190 [01:27<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 87.2901, 'train_samples_per_second': 17.276, 'train_steps_per_second': 2.177, 'train_loss': 0.6587846856368216, 'epoch': 2.0}\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"statement1\"], examples[\"statement2\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "def train_agrument_clasifier(model, statement1_train_list: list, statement2_train_list: list, labels_list: list, tokenizer):\n",
    "\n",
    "    x = list(zip(statement1_train_list, statement2_train_list))\n",
    "    y = labels_list\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.35, stratify=y)\n",
    "\n",
    "    # Do a split with stratify to preserve class distribution\n",
    "    train_data = Dataset.from_dict({\n",
    "        'statement1': [t[0] for t in x_train],\n",
    "        'statement2': [t[1] for t in x_train],\n",
    "        'label': [int(label) for label in y_train]\n",
    "    })\n",
    "\n",
    "    test_data = Dataset.from_dict({\n",
    "        'statement1': [t[0] for t in x_test],\n",
    "        'statement2': [t[1] for t in x_test],\n",
    "        'label': [int(label) for label in y_test]\n",
    "    })\n",
    "\n",
    "    train_data = train_data.map(tokenize_function, batched=True)\n",
    "    test_data = test_data.map(tokenize_function, batched=True)\n",
    "\n",
    "    # Remove original text columns (keep only tokenized inputs)\n",
    "    train_data = train_data.remove_columns([\"statement1\", \"statement2\"])\n",
    "    test_data = test_data.remove_columns([\"statement1\", \"statement2\"])\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        metric = evaluate.load(\"accuracy\")\n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        accuracy = metric.compute(predictions=predictions, references=labels)\n",
    "        return accuracy\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        num_train_epochs=num_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        warmup_steps=warmup_steps,\n",
    "        save_total_limit=2,  # limit the total amount of checkpoints, delete the older checkpoints\n",
    "        eval_steps=500, # Perform evaluation every 100 steps\n",
    "        save_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        metric_for_best_model=\"accuracy\",  # Metric to use for selecting the best model\n",
    "        greater_is_better=True,  # Whether a higher value of the metric is better\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_data,  # training data\n",
    "        eval_dataset=test_data,  # evaluation data\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model(\"./models/distilbert_agrument_classifier\")\n",
    "    tokenizer.save_pretrained(\"./models/distilbert_agrument_classifier\")\n",
    "\n",
    "    # Reload with new model\n",
    "    model = BertForSequenceClassification.from_pretrained(\"./models/distilbert_agrument_classifier\").to(device)\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"./models/distilbert_agrument_classifier\")\n",
    "\n",
    "# Load default model\n",
    "config = BertConfig.from_pretrained(bert_model_name, num_labels=len(label2id), label2id=label2id, id2label=id2label)\n",
    "model = BertForSequenceClassification.from_pretrained(bert_model_name, config=config).to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "\n",
    "# Training\n",
    "statement1_train_list=df_trainer_final[\"statement1\"].to_list()\n",
    "statement2_train_list=df_trainer_final[\"statement2\"].to_list()\n",
    "labels_list=df_trainer_final[\"label_map\"].to_list()\n",
    "\n",
    "train_agrument_clasifier(model, statement1_train_list, statement2_train_list, labels_list, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate models perfomance\n",
    "To evaluate the model we gonna give some agrument pre classified by llama:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification is Counterargument\n",
      "Classification is Counterargument\n",
      "Classification is Counterargument\n"
     ]
    }
   ],
   "source": [
    "# Function to test the sentiment of a text\n",
    "from transformers import pipeline\n",
    "nli_model = pipeline(\"text-classification\", model=\"./models/distilbert_agrument_classifier\", device=device)\n",
    "\n",
    "def test_model(statement1, statement2, model, tokenizer):\n",
    "    input_text=f\"{statement1} </s></s> {statement2}\"\n",
    "    model_result=model(input_text, truncation=True)\n",
    "    print(f\"Classification is {model_result[0]['label']}\")\n",
    "\n",
    "# Test sentiment prediction\n",
    "test_model(\"The deal is that it’s going to wipe out pre-existing conditions. And, by the way, the 200,000 people that have died on his watch, how many of those have survived? Well, there’s seven million people that contracted COVID.\"\n",
    "           ,\"And if you were here, it wouldn’t be 200, it would be two million people because you were very late on the draw. You didn’t want me to ban China, which was heavily infected. You didn’t want me to ban Europe\", \n",
    "           nli_model, tokenizer) # Counterargument\n",
    "test_model(\"People want to be safe.\",\"Those states are not doing well that are shut down right now.\", nli_model, tokenizer) # Neutral\n",
    "test_model(\"People want to be safe.\", \"Because it’s a political thing.\", nli_model, tokenizer) # Counterargument"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
